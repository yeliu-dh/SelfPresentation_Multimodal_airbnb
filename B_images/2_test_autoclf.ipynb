{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6b44d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os, sys, importlib\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "from utils import images_clf\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac681d5",
   "metadata": {},
   "source": [
    "## TEST : autoclf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35d1d85",
   "metadata": {},
   "source": [
    "## labels2emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d504e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## labels_text (prompt):\n",
    "#？\n",
    "is_default_pic_labels = {\n",
    "    \"1\": \"the official Airbnb default profile picture, a gray geometric human silhouette\",\n",
    "    \"0\": \"a normal user-uploaded profile picture\"\n",
    "}\n",
    "\n",
    "has_person_labels = {\n",
    "    \"1\": \"a photo that contains one or more people\",\n",
    "    \"0\": \"a photo without any people\"\n",
    "}\n",
    "# type_labels = {\n",
    "#     \"life\": \"a person shown in a real-life scene or activity, with visible environment or lifestyle context.\",\n",
    "#     \"pro\": \"a clean portrait or headshot focused mainly on the face, with little or no background information.\",\n",
    "#     \"UNK\": \"no visible person, or not enough information to determine lifestyle vs portrait.\"\n",
    "# }\n",
    "\n",
    "type_labels = {\n",
    "    \"life\": \n",
    "        \"a photo of a person in a visible daily scene or some activities\",\n",
    "    \"pro\": \n",
    "        \"a portrait or headshot,focused mainly on the face, with little or no background information.\",\n",
    "    \"UNK\": \n",
    "        \"an image without any people or cannot determine whether it is lifestyle or portrait\"\n",
    "}\n",
    "quality_labels = {\n",
    "    \"high\": \n",
    "        \"a clear, high-quality photo with good lighting and sharp details\",\n",
    "    \"low\": \n",
    "        \"a low-quality photo with blur, noise, poor lighting or distortion\",\n",
    "    \"UNK\": \n",
    "        \"quality cannot be determined\"\n",
    "}\n",
    "is_smiling_labels = {\n",
    "    \"1\": \"a person smiling visibly\",\n",
    "    \"0\": \"a person not smiling\",\n",
    "    \"UNK\": \"no person or cannot see their face\"\n",
    "}\n",
    "sex_labels = {\n",
    "    \"M\": \"a photo of a man\",\n",
    "    \"F\": \"a photo of a woman\",\n",
    "    \"MIX\": \"a photo with multiple people of mixed gender\",\n",
    "    \"UNK\": \"the gender of the person cannot be determined or no person present\"\n",
    "}\n",
    "labels_text = {\n",
    "    \"type\": type_labels,\n",
    "    \"quality\": quality_labels,\n",
    "    \"is_smiling\": is_smiling_labels,\n",
    "    \"sex\": sex_labels,\n",
    "    \"has_person\": has_person_labels,\n",
    "    \"is_default_pic\": is_default_pic_labels\n",
    "}\n",
    "\n",
    "# with open (\"labels/labels_text.json\",\"w\", encoding='utf-8') as f :\n",
    "#     json.dump(labels_text, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cf6dcde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\miniconda3\\envs\\airbnb_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "# embed labels :\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "device='cuda' if torch.cuda.is_available() else \"cpu\" \n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d00f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUCCES] Saved text embeddings → labels/labels_emb.npz\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1) 读取你的 labels JSON\n",
    "with open(\"labels/labels_text.json\", \"r\") as f:\n",
    "    labels_text = json.load(f)\n",
    "\n",
    "def embed_text_by_clip(text_list):\n",
    "    \"\"\"\n",
    "    text_list: list of strings\n",
    "    return: np.array of shape (len(text_list), embedding_dim)\n",
    "    label 被省去，只留下具体描述\n",
    "\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        inputs = processor(text=text_list, return_tensors=\"pt\", padding=True).to(device)\n",
    "        text_features = model.get_text_features(**inputs)  # (N, 512)\n",
    "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "        return text_features.cpu().numpy().astype(\"float32\")\n",
    "\n",
    "# 2) 生成 embedding\n",
    "labels_emb = {}\n",
    "for category, dic in labels_text.items():\n",
    "    texts = list(dic.values())  # e.g. [\"life prompt\", \"pro prompt\", \"UNK prompt\"]\n",
    "    emb = embed_text_by_clip(texts)  # shape = (num_classes, 512)\n",
    "    labels_emb[category] = emb\n",
    "\n",
    "\n",
    "# # 3) 保存到单个 npz 文件\n",
    "# np.savez(\"labels/labels_emb.npz\", **labels_emb)\n",
    "# print(\"[SUCCES] Saved text embeddings → labels/labels_emb.npz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99c9fbf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 512)\n"
     ]
    }
   ],
   "source": [
    "#检查：\n",
    "emb = np.load(\"labels/labels_emb.npz\")\n",
    "type_emb = emb[\"type\"]       # (3, 512)\n",
    "quality_emb = emb[\"quality\"] # (3, 512)\n",
    "sex_emb = emb[\"sex\"]         # (4, 512)\n",
    "print(type_emb.shape)\n",
    "# 假设有一个 image embedding img_emb (1, 512)\n",
    "\n",
    "# import numpy as np\n",
    "# pred_idx = np.argmax(np.dot(img_emb, type_emb.T))\n",
    "# pred_label = list(labels_text[\"type\"].keys())[pred_idx]\n",
    "# print(pred_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9b4c258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction：\n",
    "def zero_shot_predict(image_emb, text_emb_dict):\n",
    "    \"\"\"\n",
    "    image_emb: shape (512,)\n",
    "    text_emb_dict: {\"life\": (512,), \"pro\": (512,), ...}\n",
    "    \"\"\"\n",
    "    labels = list(text_emb_dict.keys())\n",
    "    text_embs = np.stack([text_emb_dict[k] for k in labels])  # (K, 512)\n",
    "\n",
    "    # cosine similarity\n",
    "    scores = image_emb @ text_embs.T\n",
    "    best = labels[np.argmax(scores)]\n",
    "    return best, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bd5299",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "predict on images...: 100%|██████████| 20/20 [00:00<00:00, 1632.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction on 102571900.jpg:\n",
      "Prediction on 106294215.jpg:\n",
      "Prediction on 106365215.jpg:\n",
      "Prediction on 137154154.jpg:\n",
      "Prediction on 212791574.jpg:\n",
      "Prediction on 2379345.jpg:\n",
      "Prediction on 24654560.jpg:\n",
      "Prediction on 2798386.jpg:\n",
      "Prediction on 28470251.jpg:\n",
      "Prediction on 32741638.jpg:\n",
      "Prediction on 336591839.jpg:\n",
      "Prediction on 425502119.jpg:\n",
      "Prediction on 517697918.jpg:\n",
      "Prediction on 52438163.jpg:\n",
      "Prediction on 52801103.jpg:\n",
      "Prediction on 553099349.jpg:\n",
      "Prediction on 57226046.jpg:\n",
      "Prediction on 71320446.jpg:\n",
      "Prediction on 873444.jpg:\n",
      "Prediction on 88933385.jpg:\n",
      "✅ Auto predictions on 20 images saved → annotations_SAMPLE/autoclf_predictions.json: 0.02 sec!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "start_time=time.time()\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# 文件路径\n",
    "# ------------------------\n",
    "image_emb_path = \"embeddings_SAMPLE/emb_SAMPLE.npz\"\n",
    "text_emb_path = \"labels/labels_emb.npz\"\n",
    "text_json_path = \"labels/labels_text.json\"\n",
    "output_json_path = \"annotations_SAMPLE/autoclf_predictions.json\"\n",
    "\n",
    "# ------------------------\n",
    "# 读取 embeddings\n",
    "# ------------------------\n",
    "image_embs = np.load(image_emb_path)  # keys: \"host_id.jpg\"\n",
    "text_embs_np = np.load(text_emb_path)\n",
    "with open(text_json_path, \"r\") as f:\n",
    "    labels_text = json.load(f)\n",
    "default_pic_emb=image_embs[\"336591839.jpg\"]\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# 预测函数\n",
    "# ------------------------\n",
    "def zero_shot_predict(img_emb, text_emb_dict):\n",
    "    \"\"\"\n",
    "    img_emb: np.array (512,)\n",
    "    text_emb_dict: np.array (num_classes, 512)\n",
    "    \"\"\"\n",
    "    sims = img_emb @ text_emb_dict.T        # cosine similarity\n",
    "    idx = np.argmax(sims)\n",
    "    return idx, sims\n",
    "\n",
    "\n",
    "def is_default_pic(image_emb, default_pic_emb, threshold=0.95):\n",
    "    \"\"\"\n",
    "    image_emb: np.array (512,)\n",
    "    default_emb: np.array (512,)\n",
    "    threshold: cosine similarity threshold\n",
    "    \"\"\"\n",
    "    sim = image_emb @ default_pic_emb  # cosine similarity, embeddings 已经 L2-normalized\n",
    "    return \"1\" if sim >= threshold else \"0\"\n",
    "\n",
    "# ------------------------\n",
    "# 遍历每张图片\n",
    "# ------------------------\n",
    "predictions = {}  # 存储结果\n",
    "for fname in tqdm(image_embs.files, desc='predict on images...'):\n",
    "    img_emb = image_embs[fname]\n",
    "    \n",
    "    # pred = {}\n",
    "    # for category, text_emb_np in text_embs_np.items():\n",
    "    #     idx, sims = zero_shot_predict(img_emb, text_emb_np)\n",
    "    #     label_keys = list(labels_text[category].keys())\n",
    "    #     pred_label = label_keys[idx]\n",
    "    #     pred[category] = pred_label\n",
    "    \n",
    "    # predictions[fname] = pred\n",
    "    pred = {}\n",
    "    for category, text_emb_np in text_embs_np.items():\n",
    "        if category == \"is_default_pic\":\n",
    "            pred_label = is_default_pic(img_emb, default_pic_emb)\n",
    "        else:\n",
    "            idx, sims = zero_shot_predict(img_emb, text_emb_np)\n",
    "            label_keys = list(labels_text[category].keys())\n",
    "            pred_label = label_keys[idx]\n",
    "        pred[category] = pred_label\n",
    "\n",
    "    predictions[fname] = pred\n",
    "\n",
    "# ------------------------\n",
    "# 保存 JSON\n",
    "# ------------------------\n",
    "with open(output_json_path, \"w\") as f:\n",
    "    json.dump(predictions, f, indent=2)\n",
    "\n",
    "\n",
    "end_time=time.time()\n",
    "print(f\"✅ Auto predictions on {len(image_embs)} images saved → {output_json_path}: {end_time-start_time:.2f} sec!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7250143a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980d74ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a6329c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4054833",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "airbnb_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
