{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6b44d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os, sys, importlib\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "from utils import images_clf\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac681d5",
   "metadata": {},
   "source": [
    "## TEST : autoclf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35d1d85",
   "metadata": {},
   "source": [
    "## labels2emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "23d504e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## labels_text (prompt):\n",
    "#Ôºü\n",
    "# is_default_pic_labels = {\n",
    "#     \"1\": \"the official Airbnb default profile picture, a gray geometric human silhouette\",\n",
    "#     \"0\": \"a normal user-uploaded profile picture\"\n",
    "# }\n",
    "\n",
    "# has_person_labels = {\n",
    "#     \"1\": \"a photo that contains one or more people\",\n",
    "#     \"0\": \"a photo without any people\"\n",
    "# }\n",
    "# type_labels = {\n",
    "#     \"life\": \"a person shown in a real-life scene or activity, with visible environment or lifestyle context.\",\n",
    "#     \"pro\": \"a clean portrait or headshot focused mainly on the face, with little or no background information.\",\n",
    "#     \"UNK\": \"no visible person, or not enough information to determine lifestyle vs portrait.\"\n",
    "# }\n",
    "\n",
    "type_labels = {\n",
    "    \"life\": \n",
    "        \"a photo of a person in a visible daily scene or some activities\",\n",
    "    \"pro\": \n",
    "        \"a portrait or headshot,focused mainly on the face, with little or no background information.\",\n",
    "    \"UNK\": \n",
    "        \"an image without any people or cannot determine whether it is lifestyle or portrait\"\n",
    "}\n",
    "# quality_labels = {\n",
    "#     \"high\": \n",
    "#         \"a clear, high-quality photo with good lighting and sharp details\",\n",
    "#     \"low\": \n",
    "#         \"a low-quality photo with blur, noise, poor lighting or distortion\",\n",
    "#     \"UNK\": \n",
    "#         \"quality cannot be determined\"\n",
    "# }\n",
    "is_smiling_labels = {\n",
    "    \"1\": \"a person smiling visibly\",\n",
    "    \"0\": \"a person not smiling\",\n",
    "    \"UNK\": \"no person or cannot see their face\"\n",
    "}\n",
    "sex_labels = {\n",
    "    \"M\": \"a photo of a man\",\n",
    "    \"F\": \"a photo of a woman\",\n",
    "    \"MIX\": \"a photo with multiple people of mixed gender\",\n",
    "    \"UNK\": \"the gender of the person cannot be determined or no person present\"\n",
    "}\n",
    "labels_text = {\n",
    "    # \"has_person\": has_person_labels,\n",
    "    # \"is_default_pic\": is_default_pic_labels,\n",
    "    \"type\": type_labels,\n",
    "    # \"quality\": quality_labels,\n",
    "    \"is_smiling\": is_smiling_labels,\n",
    "    \"sex\": sex_labels   \n",
    "}\n",
    "\n",
    "\n",
    "with open (\"labels/labels_text.json\",\"w\", encoding='utf-8') as f :\n",
    "    json.dump(labels_text, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d03d26",
   "metadata": {},
   "source": [
    "## emb text prompt V1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0cf6dcde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\miniconda3\\envs\\airbnb_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "# embed labels :\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "device='cuda' if torch.cuda.is_available() else \"cpu\" \n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d00f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUCCES] Saved text embeddings ‚Üí labels/labels_emb.npz\n"
     ]
    }
   ],
   "source": [
    "# 1) ËØªÂèñ‰Ω†ÁöÑ labels JSON\n",
    "with open(\"labels/labels_text.json\", \"r\") as f:\n",
    "    labels_text = json.load(f)\n",
    "\n",
    "def embed_text_by_clip(text_list):\n",
    "    \"\"\"\n",
    "    text_list: list of strings\n",
    "    return: np.array of shape (len(text_list), embedding_dim)\n",
    "    label Ë¢´ÁúÅÂéªÔºåÂè™Áïô‰∏ãÂÖ∑‰ΩìÊèèËø∞\n",
    "\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        inputs = processor(text=text_list, return_tensors=\"pt\", padding=True).to(device)\n",
    "        text_features = model.get_text_features(**inputs)  # (N, 512)\n",
    "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "        return text_features.cpu().numpy().astype(\"float32\")\n",
    "\n",
    "# 2) ÁîüÊàê embedding\n",
    "labels_emb = {}\n",
    "for category, dic in labels_text.items():\n",
    "    texts = list(dic.values())  # e.g. [\"life prompt\", \"pro prompt\", \"UNK prompt\"]\n",
    "    emb = embed_text_by_clip(texts)  # shape = (num_classes, 512)\n",
    "    labels_emb[category] = emb\n",
    "\n",
    "\n",
    "# 3) ‰øùÂ≠òÂà∞Âçï‰∏™ npz Êñá‰ª∂\n",
    "np.savez(\"labels/labels_emb.npz\", **labels_emb)\n",
    "print(\"[SUCCES] Saved text embeddings ‚Üí labels/labels_emb.npz\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedbdcbe",
   "metadata": {},
   "source": [
    "## emb text prompt V2 : few-shot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a3e9468f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUCCES] Saved ‚Üí labels/labels/labels_emb_txt-img.pkl\n"
     ]
    }
   ],
   "source": [
    "import json, pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else \"cpu\"\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "\n",
    "def embed_text_by_clip(text_list):\n",
    "    with torch.no_grad():\n",
    "        inputs = processor(text=text_list, return_tensors=\"pt\", padding=True).to(device)\n",
    "        text_features = model.get_text_features(**inputs)\n",
    "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "        return text_features.cpu().numpy().astype(\"float32\")\n",
    "\n",
    "\n",
    "# ---- 1) ËØªÂèñÊñáÊú¨Ê†áÁ≠æ ----\n",
    "with open(\"labels/labels_text.json\", \"r\") as f:\n",
    "    labels_text = json.load(f)\n",
    "\n",
    "# ---- 2) ÂØπÊâÄÊúâÊñáÊú¨ÁîüÊàê embedding ----\n",
    "labels_emb = {}\n",
    "for category, dic in labels_text.items():\n",
    "    texts = list(dic.values())\n",
    "    emb = embed_text_by_clip(texts)\n",
    "    # ÊåâÂ≠óÂÖ∏È°∫Â∫èÂØπÂ∫îÂõû keys\n",
    "    labels_emb[category] = {\n",
    "        cls_name: emb[i:i+1]\n",
    "        for i, cls_name in enumerate(dic.keys())\n",
    "    }\n",
    "\n",
    "# ---- 3) Áªô ‚Äútype‚Äù Á±ªÂà´Âä†ÂÖ• example images embeddings ----\n",
    "\n",
    "# Âä†ËΩΩ image embeddings\n",
    "img_embeddings_path=\"embeddings_SAMPLE/emb_SAMPLE.npz\"\n",
    "img_embs = np.load(img_embeddings_path)\n",
    "\n",
    "# ‰Ω†Ëá™Â∑±Êåë‰∏Ä‰∫õÁ§∫‰æãÂõæÁâáÔºö\n",
    "life_imgs = [\"106365215.jpg\", \"517697918.jpg\"]\n",
    "pro_imgs  = [\"102571900.jpg\",\"71320446.jpg\"]\n",
    "unk_imgs=[\"52801103.jpg\",\"425502119.jpg\"]\n",
    "\n",
    "final_type_emb = {}\n",
    "for cls_name, emb_text in labels_emb[\"type\"].items():\n",
    "    all_embs = [emb_text]  # start with text prompt emb\n",
    "\n",
    "    if cls_name == \"life\":\n",
    "        for f in life_imgs:\n",
    "            all_embs.append(img_embs[f][None])\n",
    "    if cls_name == \"pro\":\n",
    "        for f in pro_imgs:\n",
    "            all_embs.append(img_embs[f][None])\n",
    "    if cls_name=='UNK':\n",
    "        for f in unk_imgs:\n",
    "            all_embs.append(img_embs[f][None])\n",
    "\n",
    "    final_type_emb[cls_name] = np.vstack(all_embs)\n",
    "\n",
    "# Ë¶ÜÁõñÂéüÊù•ÁöÑ type\n",
    "labels_emb[\"type\"] = final_type_emb\n",
    "\n",
    "# ---- 4) ‰øùÂ≠ò‰∏∫ pklÔºàÊîØÊåÅÂµåÂ•óÁªìÊûÑÔºâ ----\n",
    "with open(\"labels/labels_emb_txt-img.pkl\", \"wb\") as f:\n",
    "    pickle.dump(labels_emb, f)\n",
    "\n",
    "print(\"[SUCCES] Saved ‚Üí labels/labels/labels_emb_txt-img.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be245db0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c9fbf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 512)\n"
     ]
    }
   ],
   "source": [
    "#Ê£ÄÊü•Ôºö\n",
    "emb = np.load(\"labels/labels_emb.npz\")\n",
    "type_emb = emb[\"type\"]       # (3, 512)\n",
    "quality_emb = emb[\"quality\"] # (3, 512)\n",
    "sex_emb = emb[\"sex\"]         # (4, 512)\n",
    "print(type_emb.shape)\n",
    "# ÂÅáËÆæÊúâ‰∏Ä‰∏™ image embedding img_emb (1, 512)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ea523c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca41ee37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d587d4ec",
   "metadata": {},
   "source": [
    "## PREDICTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bd5299",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "predict on images...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 5054.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Auto predictions on 20 images saved ‚Üí annotations_SAMPLE/autoclf_predictions.json: 0.01 sec!\n",
      "{'102571900.jpg': {'is_default_pic': '0', 'type': 'pro', 'is_smiling': '1', 'sex': 'F'}, '106294215.jpg': {'is_default_pic': '0', 'type': 'life', 'is_smiling': 'UNK', 'sex': 'UNK'}, '106365215.jpg': {'is_default_pic': '0', 'type': 'life', 'is_smiling': '1', 'sex': 'M'}, '137154154.jpg': {'is_default_pic': '0', 'type': 'life', 'is_smiling': 'UNK', 'sex': 'F'}, '212791574.jpg': {'is_default_pic': '0', 'type': 'UNK', 'is_smiling': '0', 'sex': 'UNK'}, '2379345.jpg': {'is_default_pic': '0', 'type': 'UNK', 'is_smiling': '0', 'sex': 'UNK'}, '24654560.jpg': {'is_default_pic': '0', 'type': 'UNK', 'is_smiling': 'UNK', 'sex': 'MIX'}, '2798386.jpg': {'is_default_pic': '0', 'type': 'pro', 'is_smiling': '1', 'sex': 'M'}, '28470251.jpg': {'is_default_pic': '0', 'type': 'pro', 'is_smiling': '1', 'sex': 'F'}, '32741638.jpg': {'is_default_pic': '0', 'type': 'pro', 'is_smiling': '1', 'sex': 'F'}, '336591839.jpg': {'is_default_pic': '1', 'type': 'UNK', 'is_smiling': 'UNK', 'sex': 'UNK'}, '425502119.jpg': {'is_default_pic': '0', 'type': 'UNK', 'is_smiling': 'UNK', 'sex': 'UNK'}, '517697918.jpg': {'is_default_pic': '0', 'type': 'life', 'is_smiling': 'UNK', 'sex': 'UNK'}, '52438163.jpg': {'is_default_pic': '0', 'type': 'UNK', 'is_smiling': '0', 'sex': 'MIX'}, '52801103.jpg': {'is_default_pic': '0', 'type': 'UNK', 'is_smiling': 'UNK', 'sex': 'UNK'}, '553099349.jpg': {'is_default_pic': '0', 'type': 'UNK', 'is_smiling': '0', 'sex': 'M'}, '57226046.jpg': {'is_default_pic': '0', 'type': 'pro', 'is_smiling': '0', 'sex': 'M'}, '71320446.jpg': {'is_default_pic': '0', 'type': 'pro', 'is_smiling': '1', 'sex': 'M'}, '873444.jpg': {'is_default_pic': '0', 'type': 'UNK', 'is_smiling': '1', 'sex': 'F'}, '88933385.jpg': {'is_default_pic': '0', 'type': 'UNK', 'is_smiling': 'UNK', 'sex': 'F'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "start_time=time.time()\n",
    "\n",
    "# ------------------------\n",
    "# Êñá‰ª∂Ë∑ØÂæÑ\n",
    "# ------------------------\n",
    "image_emb_path = \"embeddings_SAMPLE/emb_SAMPLE.npz\"\n",
    "text_emb_path = \"labels/labels_emb_txt-img.pkl\"\n",
    "text_json_path = \"labels/labels_text.json\"\n",
    "output_json_path = \"annotations_SAMPLE/autoclf_predictions.json\"\n",
    "\n",
    "# ------------------------\n",
    "# ËØªÂèñ embeddings\n",
    "# ------------------------\n",
    "image_embs = np.load(image_emb_path)  # keys: \"host_id.jpg\"\n",
    "\n",
    "with open(text_emb_path, \"rb\") as f:\n",
    "    labels_emb = pickle.load(f)\n",
    "\n",
    "with open(text_json_path, \"r\") as f:\n",
    "    labels_text = json.load(f)\n",
    "default_pic_emb=image_embs[\"336591839.jpg\"]\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# È¢ÑÊµãÂáΩÊï∞\n",
    "# ------------------------\n",
    "def is_default_pic(image_emb, default_pic_emb, threshold=0.95):\n",
    "    \"\"\"\n",
    "    image_emb: np.array (512,)\n",
    "    default_emb: np.array (512,)\n",
    "    threshold: cosine similarity threshold\n",
    "    \"\"\"\n",
    "    sim = image_emb @ default_pic_emb  # cosine similarity, embeddings Â∑≤Áªè L2-normalized\n",
    "    return \"1\" if sim >= threshold else \"0\"\n",
    "\n",
    "def zero_shot_predict(img_emb, text_emb_dict):\n",
    "    \"\"\"\n",
    "    img_emb: np.array (512,)\n",
    "    text_emb_dict: np.array (num_classes, 512)\n",
    "    \"\"\"\n",
    "    sims = img_emb @ text_emb_dict.T        # cosine similarity\n",
    "    idx = np.argmax(sims)\n",
    "    return idx, sims\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def few_shot_predict(img_emb, support_dict):\n",
    "    \"\"\"\n",
    "    img_emb: (512,)\n",
    "    support_dict: dict[class_name -> (N,512)]  \n",
    "        ÊØè‰∏™Á±ªÂà´ÂèØ‰ª•ÊúâÊñáÊú¨embeddingÂíåÁ§∫‰æãÂõæÁâáembedding\n",
    "    ËøîÂõûÔºöÁ±ªÂà´Âêç\n",
    "    \"\"\"\n",
    "    best_cls = None\n",
    "    best_sim = -999\n",
    "\n",
    "    for cls_name, emb_set in support_dict.items():  # emb_set shape = (K,512)\n",
    "        sims = img_emb @ emb_set.T                  # ‚Üí (K,)\n",
    "        score = sims.max()                          # ÂèñÊúÄÂ§ßÁõ∏‰ººÂ∫¶\n",
    "\n",
    "        if score > best_sim:\n",
    "            best_sim = score\n",
    "            best_cls = cls_name\n",
    "\n",
    "    return best_cls\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# ÈÅçÂéÜÊØèÂº†ÂõæÁâá\n",
    "# ------------------------\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "predictions = {}\n",
    "\n",
    "for fname in tqdm(image_embs.files, desc='predict on images...'):\n",
    "    img_emb = image_embs[fname]\n",
    "\n",
    "    pred = {}\n",
    "\n",
    "    # ---- 1) ÈªòËÆ§Â§¥ÂÉèÂà§Êñ≠ ----\n",
    "    pred[\"is_default_pic\"] = is_default_pic(img_emb, default_pic_emb)\n",
    "\n",
    "    # Â¶ÇÊûúÊòØÈªòËÆ§Â§¥ÂÉèÔºåÁõ¥Êé•Ë¶ÜÁõñÂÖ∂‰ªñÊ†áÁ≠æ\n",
    "    if pred[\"is_default_pic\"] == \"1\":\n",
    "        pred.update({\n",
    "            \"type\": \"UNK\",\n",
    "            \"is_smiling\": \"UNK\",\n",
    "            \"sex\": \"UNK\"\n",
    "        })\n",
    "        predictions[fname] = pred\n",
    "        continue\n",
    "\n",
    "    # ---- 2) few-shot È¢ÑÊµãÂÖ∂‰ªñÁ±ªÂà´ ----\n",
    "    for category in labels_emb.keys():\n",
    "        if category == \"is_default_pic\":\n",
    "            continue #Ë∑≥ËøáÂΩìÂâçÂæ™ÁéØÁöÑÂâ©‰ΩôÈÉ®ÂàÜÔºåÁõ¥Êé•ËøõÂÖ•‰∏ã‰∏ÄÊ¨°Âæ™ÁéØ\n",
    "\n",
    "        support_dict = labels_emb[category]  # dict[class_name -> (N,512)]\n",
    "        best_cls = few_shot_predict(img_emb, support_dict)\n",
    "\n",
    "        pred[category] = best_cls\n",
    "\n",
    "    predictions[fname] = pred\n",
    "\n",
    "# ------------------------\n",
    "# ‰øùÂ≠ò JSON\n",
    "# ------------------------\n",
    "with open(output_json_path, \"w\") as f:\n",
    "    json.dump(predictions, f, indent=2)\n",
    "\n",
    "end_time=time.time()\n",
    "print(f\"‚úÖ Auto predictions on {len(image_embs)} images saved ‚Üí {output_json_path}: {end_time-start_time:.2f} sec!\")\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53b037a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15730d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a6329c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a6a709b0",
   "metadata": {},
   "source": [
    "## DETECTION:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f4054833",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "detecting has_person and picture quality...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 20.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUCCES] 20 detection results saved to annotations_SAMPLE/detections.json :1.00 sec!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'102571900.jpg': {'has_person': '1', 'quality': 'high'},\n",
       " '106294215.jpg': {'has_person': '0', 'quality': 'high'},\n",
       " '106365215.jpg': {'has_person': '1', 'quality': 'high'},\n",
       " '137154154.jpg': {'has_person': '1', 'quality': 'high'},\n",
       " '212791574.jpg': {'has_person': '0', 'quality': 'high'},\n",
       " '2379345.jpg': {'has_person': '0', 'quality': 'high'},\n",
       " '24654560.jpg': {'has_person': '1', 'quality': 'high'},\n",
       " '2798386.jpg': {'has_person': '1', 'quality': 'high'},\n",
       " '28470251.jpg': {'has_person': '1', 'quality': 'high'},\n",
       " '32741638.jpg': {'has_person': '0', 'quality': 'low'},\n",
       " '336591839.jpg': {'has_person': '0', 'quality': 'low'},\n",
       " '425502119.jpg': {'has_person': '0', 'quality': 'high'},\n",
       " '517697918.jpg': {'has_person': '1', 'quality': 'high'},\n",
       " '52438163.jpg': {'has_person': '1', 'quality': 'high'},\n",
       " '52801103.jpg': {'has_person': '0', 'quality': 'low'},\n",
       " '553099349.jpg': {'has_person': '1', 'quality': 'low'},\n",
       " '57226046.jpg': {'has_person': '1', 'quality': 'high'},\n",
       " '71320446.jpg': {'has_person': '1', 'quality': 'high'},\n",
       " '873444.jpg': {'has_person': '1', 'quality': 'high'},\n",
       " '88933385.jpg': {'has_person': '0', 'quality': 'high'}}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 1. Âä†ËΩΩ YOLO Ê®°ÂûãÔºàÊúÄËΩªÈáè CPU/GPU ÈÄöÂêÉÔºâ\n",
    "# -------------------------------------------------\n",
    "model = YOLO(\"yolov8n.pt\")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 2. quality Âà§Êñ≠ÔºàÂü∫‰∫éÊúÄÁü≠Ëæπ pxÔºâ\n",
    "# -------------------------------------------------\n",
    "# def get_quality_label(img_path,short_edge_threshold=150):\n",
    "#     try:\n",
    "#         img = Image.open(img_path)\n",
    "#         w, h = img.size\n",
    "#         short_edge = min(w, h)\n",
    "\n",
    "#         # Ê†πÊçÆ Airbnb Â§¥ÂÉèÁâπÁÇπÔºö200px ‰ª•‰∏ãÂá†‰πéËÇØÂÆö low\n",
    "#         if short_edge >= short_edge_threshold:\n",
    "#             return \"high\"\n",
    "#         else:\n",
    "#             return \"low\"\n",
    "#     except:\n",
    "#         return \"UNK\"\n",
    "\n",
    "def get_quality_label(img_path, blur_threshold=100.0):\n",
    "    \"\"\"\n",
    "    Ê†πÊçÆÂõæÂÉèÊòØÂê¶Ê®°Á≥äÂà§Êñ≠Ë¥®Èáè\n",
    "    img_path: ÂõæÁâáË∑ØÂæÑ\n",
    "    blur_threshold: Laplacian ÊñπÂ∑ÆÈòàÂÄºÔºåÂÄºË∂äÂ∞èË∂äÊ®°Á≥ä\n",
    "    return: \"high\" Êàñ \"low\"\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Áî® PIL ËØªÂèñÂπ∂ËΩ¨‰∏∫ÁÅ∞Â∫¶\n",
    "        img = Image.open(img_path).convert(\"L\")  \n",
    "        img_np = np.array(img)\n",
    "\n",
    "        # Laplacian ÊñπÂ∑Æ\n",
    "        lap_var = cv2.Laplacian(img_np, cv2.CV_64F).var()\n",
    "\n",
    "        if lap_var >= blur_threshold:\n",
    "            return \"high\"\n",
    "        else:\n",
    "            return \"low\"\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to process {img_path}: {e}\")\n",
    "        return \"low\"\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 3. YOLO Âà§Êñ≠ has_person\n",
    "# -------------------------------------------------\n",
    "def detect_has_person(img_path):\n",
    "    try:\n",
    "        # results = model(img_path)[0]  # first result\n",
    "        results = model(img_path, verbose=False)[0]  # üëà ÂÖ≥Èó≠ÊâÄÊúâÊó•ÂøóËæìÂá∫\n",
    "        for box in results.boxes:\n",
    "            cls = int(box.cls[0])\n",
    "            if results.names[cls] == \"person\":\n",
    "                return \"1\"\n",
    "        return \"0\"\n",
    "    except:\n",
    "        return \"UNK\"\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 4. main pipeline\n",
    "# -------------------------------------------------\n",
    "def detect_person_and_quality(\n",
    "        images_folder=\"images_SAMPLE\",\n",
    "        save_json=\"annotations_SAMPLE/detections.json\"\n",
    "    ):\n",
    "    start_time=time.time()\n",
    "\n",
    "    os.makedirs(os.path.dirname(save_json), exist_ok=True)\n",
    "    records = {}\n",
    "\n",
    "    image_files = [f for f in os.listdir(images_folder) if f.lower().endswith((\".jpg\",\".jpeg\",\".png\"))]\n",
    "\n",
    "    # print(f\"[INFO] Found {len(image_files)} images in {images_folder}\")\n",
    "\n",
    "    for fname in tqdm(image_files, desc=\"detecting has_person and picture quality...\"):\n",
    "        fpath = os.path.join(images_folder, fname)\n",
    "\n",
    "        # YOLO: detect person\n",
    "        has_person = detect_has_person(fpath)\n",
    "\n",
    "        # quality: size-based\n",
    "        quality = get_quality_label(fpath)\n",
    "\n",
    "        records[fname] = {\n",
    "            \"has_person\": has_person,\n",
    "            \"quality\": quality\n",
    "        }\n",
    "\n",
    "        # print(f\"[OK] {fname} ‚Üí has_person={has_person}, quality={quality}\")\n",
    "\n",
    "    # save json\n",
    "    with open(save_json, \"w\") as f:\n",
    "        json.dump(records, f, indent=2)\n",
    "    end_time=time.time()\n",
    "\n",
    "    print(f\"[SUCCES] {len(image_files)} detection results saved to {save_json} :{end_time-start_time:.2f} sec!\")\n",
    "    return records\n",
    "\n",
    "# # -------------------------------------------------\n",
    "# # Run\n",
    "# # -------------------------------------------------\n",
    "# if __name__ == \"__main__\":\n",
    "#     run_yolo_pipeline()\n",
    "\n",
    "\n",
    "detect_person_and_quality(\n",
    "        images_folder=\"images_SAMPLE\",\n",
    "        save_json=\"annotations_SAMPLE/detections.json\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f54609f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5082dbe1",
   "metadata": {},
   "source": [
    "## merge prediction and detection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476b6a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**PROCESS** merging detections and predictions...\n",
      "[INFO] keys alignement: True\n",
      "[SAVE] merged and overrided auto-annotations saved in annotations_SAMPLE/auto_annotations.json!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def read_json(json_path):\n",
    "    with open(json_path, 'r', encoding='utf-8')as f:\n",
    "        data=json.load(f)\n",
    "    return data\n",
    "\n",
    "def apply_override_rules (annos):\n",
    "    \"\"\"\n",
    "    {\n",
    "    \"is_default_pic\": \"1\",\n",
    "    \"type\": \"UNK\",\n",
    "    \"is_smiling\": \"UNK\",\n",
    "    \"sex\": \"UNK\",\n",
    "    \"has_person\": \"0\",\n",
    "    \"quality\": \"UNK\"\n",
    "    },\n",
    "    {...}\n",
    "\n",
    "    \"\"\"\n",
    "    if annos['is_default_pic']==1:\n",
    "        return {\n",
    "            \"is_default_pic\": \"1\",\n",
    "            \"type\": \"UNK\",\n",
    "            \"is_smiling\": \"UNK\",\n",
    "            \"sex\": \"UNK\",\n",
    "            \"has_person\": \"0\",\n",
    "            \"quality\": \"low\"\n",
    "            }       \n",
    "\n",
    "    elif annos['has_person']==0:#no person\n",
    "        return { \n",
    "            \"is_default_pic\": \"0\",\n",
    "            \"type\": \"UNK\",\n",
    "            \"is_smiling\": \"UNK\",\n",
    "            \"sex\": \"UNK\",\n",
    "            \"has_person\": \"0\",\n",
    "            \"quality\": annos['quality']\n",
    "        }\n",
    "        \n",
    "    else :\n",
    "        return annos\n",
    "\n",
    "\n",
    "def merge_annotations (detection_path, prediction_path, auto_annotations_path):\n",
    "    print(\"**PROCESS** merging detections and predictions...\")\n",
    "    detections=read_json(detection_path)\n",
    "    predictions=read_json(prediction_path)\n",
    "    print(f\"[INFO] keys alignement: {predictions.keys()==detections.keys()}\")# check\n",
    "\n",
    "    merged = {}\n",
    "    for fname in predictions.keys():\n",
    "        merged[fname] = {}  # Êñ∞Âª∫Â≠êÂ≠óÂÖ∏\n",
    "\n",
    "        # 1. ÂÖàÊîæ CLIP/few-shot È¢ÑÊµãÁªìÊûú\n",
    "        if fname in predictions:\n",
    "            merged[fname].update(predictions[fname])\n",
    "\n",
    "        # 2. ÂÜçÊîæ YOLO detections\n",
    "        if fname in detections:\n",
    "            merged[fname].update(detections[fname])\n",
    "\n",
    "        # 3. OVERRIDE:\n",
    "        updated_annos=apply_override_rules(merged[fname])\n",
    "        merged[fname]=updated_annos\n",
    "\n",
    "    # 4. ‰øùÂ≠ò‰∏∫ json\n",
    "    with open(auto_annotations_path, \"w\") as f:\n",
    "        json.dump(merged, f, indent=2)\n",
    "    print(f\"[SAVE] merged and overrided auto-annotations saved in {auto_annotations_path}!\\n\")\n",
    "    return \n",
    "\n",
    "\n",
    "detection_path=\"annotations_SAMPLE/detections.json\"\n",
    "prediction_path=\"annotations_SAMPLE/autoclf_predictions.json\"\n",
    "auto_annotations_path=\"annotations_SAMPLE/auto_annotations.json\"\n",
    "merge_annotations (detection_path, prediction_path, auto_annotations_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7190d76",
   "metadata": {},
   "source": [
    "## evaluatation of autoclf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cb830cee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dimension</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>macro_f1</th>\n",
       "      <th>micro_f1</th>\n",
       "      <th>UNK_count</th>\n",
       "      <th>low_performance_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>is_default_pic</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>has_person</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.890110</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>type</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.707143</td>\n",
       "      <td>0.70</td>\n",
       "      <td>6</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>quality</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.438272</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>is_smiling</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.660131</td>\n",
       "      <td>0.75</td>\n",
       "      <td>9</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sex</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.918831</td>\n",
       "      <td>0.90</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        dimension  accuracy  macro_f1  micro_f1  UNK_count  \\\n",
       "0  is_default_pic      1.00  1.000000      1.00          0   \n",
       "1      has_person      0.90  0.890110      0.90          0   \n",
       "2            type      0.70  0.707143      0.70          6   \n",
       "3         quality      0.70  0.438272      0.70          1   \n",
       "4      is_smiling      0.75  0.660131      0.75          9   \n",
       "5             sex      0.90  0.918831      0.90          7   \n",
       "\n",
       "   low_performance_flag  \n",
       "0                 False  \n",
       "1                 False  \n",
       "2                 False  \n",
       "3                 False  \n",
       "4                 False  \n",
       "5                 False  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## evaluation of clip:\n",
    "import json\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "#load annotations :\n",
    "ls_annotations_path=\"annotations_SAMPLE/ls_annotations.json\"#y_true\n",
    "auto_annotations_path=\"annotations_SAMPLE/auto_annotations.json\"#y_pred\n",
    "ls_annotations_brut=read_json(ls_annotations_path)\n",
    "auto_annotations=read_json(auto_annotations_path)\n",
    "\n",
    "# parse ls_annotations:\n",
    "\n",
    "def ls_to_dict(ls_json):\n",
    "    \"\"\"\n",
    "    ls_json: list, LabelStudio ÂØºÂá∫ json\n",
    "    return: dict {filename: {dim:value}}\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    for item in ls_json:\n",
    "        # ÊâæÂà∞ÂõæÁâáÂêç\n",
    "        fname = item.get(\"data\", {}).get(\"filename\") or item.get(\"file_upload\")\n",
    "        if not fname:\n",
    "            continue\n",
    "        \n",
    "        # ÂèñÁ¨¨‰∏ÄÊù°Ê†áÊ≥®Ôºàannotations ÂèØËÉΩÊúâÂ§ö‰∫∫Ê†áÊ≥®ÔºåÂèñÁ¨¨‰∏Ä‰∏™Âç≥ÂèØÔºâ\n",
    "        ann_list = item.get(\"annotations\", [])\n",
    "        if not ann_list:\n",
    "            continue\n",
    "        \n",
    "        ann_result = ann_list[0].get(\"result\", [])\n",
    "        labels = {}\n",
    "        for r in ann_result:\n",
    "            dim = r.get(\"from_name\")\n",
    "            value_dict = r.get(\"value\", {})\n",
    "            # choices ÊòØ LabelStudio ÈªòËÆ§Ê†ºÂºè\n",
    "            if \"choices\" in value_dict and len(value_dict[\"choices\"]) > 0:\n",
    "                labels[dim] = value_dict[\"choices\"][0]\n",
    "            else:\n",
    "                labels[dim] = \"UNK\"  # Ê≤°ÊúâÊ†áÊ≥®Â∞±Â°´ UNK\n",
    "\n",
    "        out[fname] = labels\n",
    "    return out\n",
    "\n",
    "ls_annotations = ls_to_dict(ls_annotations_brut)\n",
    "# print(ls_annotations)\n",
    "\n",
    "\n",
    "# evaluation:\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "import json\n",
    "\n",
    "def summarize_classification(auto_annos, ls_annos, label_dims=None):\n",
    "    \"\"\"\n",
    "    auto_annos: dict {filename: {dim: label}}\n",
    "    ls_annos: dict {filename: {dim: label}}  # ground truth\n",
    "    label_dims: list of dimensions to evaluate, e.g. [\"type\",\"quality\",...]\n",
    "    F1:\n",
    "    ÂÆèÂπ≥Âùá (macro)ÔºöÊØè‰∏™Á±ªÂà´ F1 Âπ≥Âùá ‚Üí Á±ªÂà´‰∏çÂπ≥Ë°°ÊïèÊÑü\n",
    "    ÂæÆÂπ≥Âùá (micro)ÔºöÂÖ®Â±Ä TP/FP/FN ËÆ°ÁÆó ‚Üí ÂØπÊ†∑Êú¨ÈáèÊïèÊÑü\n",
    "\n",
    "\n",
    "    ËøîÂõû DataFrameÔºåÂèØÁõ¥Êé•ÊâìÂç∞Êàñ‰øùÂ≠ò CSV\n",
    "    \"\"\"\n",
    "    if label_dims is None:\n",
    "        # ‰ªé ground truth ÂèñÊâÄÊúâÁª¥Â∫¶\n",
    "        label_dims = list(next(iter(ls_annos.values())).keys())\n",
    "    \n",
    "    records = []\n",
    "    \n",
    "    for dim in label_dims:\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        unk_count = 0\n",
    "        for fname in ls_annos:\n",
    "            true_label = ls_annos[fname].get(dim, \"UNK\")\n",
    "            pred_label = auto_annos.get(fname, {}).get(dim, \"UNK\")\n",
    "            y_true.append(true_label)\n",
    "            y_pred.append(pred_label)\n",
    "            if true_label == \"UNK\":\n",
    "                unk_count += 1\n",
    "        \n",
    "        # classification report\n",
    "        report = classification_report(y_true, y_pred, output_dict=True, zero_division=0)\n",
    "    \n",
    "        # macro avg f1\n",
    "        # macro_f1 = report[\"macro avg\"][\"f1-score\"]\n",
    "\n",
    "        # macro f1\n",
    "        macro_f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "\n",
    "        # micro f1\n",
    "        micro_f1 = f1_score(y_true, y_pred, average='micro', zero_division=0)\n",
    "    \n",
    "        # accuracy\n",
    "        # acc = report[\"accuracy\"]\n",
    "        acc = sum([t==p for t,p in zip(y_true, y_pred)]) / len(y_true)\n",
    "\n",
    "\n",
    "        records.append({\n",
    "            \"dimension\": dim,\n",
    "            \"accuracy\": acc,\n",
    "            \"macro_f1\": macro_f1,\n",
    "            \"micro_f1\":micro_f1,\n",
    "            \"UNK_count\": unk_count,\n",
    "            \"low_performance_flag\": micro_f1 < 0.7  # Ê†áËÆ∞ F1 < 0.7\n",
    "        })\n",
    "    \n",
    "    df_summary = pd.DataFrame(records)\n",
    "    return df_summary\n",
    "\n",
    "\n",
    "dims = [\"is_default_pic\",\"has_person\",\"type\", \"quality\",\"is_smiling\",\"sex\"]\n",
    "df_summary=summarize_classification(auto_annos=auto_annotations, ls_annos=ls_annotations, label_dims=dims)\n",
    "display(df_summary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7ee3dd2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------is_default_pic-----------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        19\n",
      "           1       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           1.00        20\n",
      "   macro avg       1.00      1.00      1.00        20\n",
      "weighted avg       1.00      1.00      1.00        20\n",
      " \n",
      "\n",
      "--------------------has_person-----------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      1.00      0.86         6\n",
      "           1       1.00      0.86      0.92        14\n",
      "\n",
      "    accuracy                           0.90        20\n",
      "   macro avg       0.88      0.93      0.89        20\n",
      "weighted avg       0.93      0.90      0.90        20\n",
      " \n",
      "\n",
      "--------------------type-----------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         UNK       0.60      1.00      0.75         6\n",
      "        life       1.00      0.40      0.57        10\n",
      "         pro       0.67      1.00      0.80         4\n",
      "\n",
      "    accuracy                           0.70        20\n",
      "   macro avg       0.76      0.80      0.71        20\n",
      "weighted avg       0.81      0.70      0.67        20\n",
      " \n",
      "\n",
      "--------------------quality-----------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         UNK       0.00      0.00      0.00         1\n",
      "        high       0.69      1.00      0.81        11\n",
      "         low       0.75      0.38      0.50         8\n",
      "\n",
      "    accuracy                           0.70        20\n",
      "   macro avg       0.48      0.46      0.44        20\n",
      "weighted avg       0.68      0.70      0.65        20\n",
      " \n",
      "\n",
      "--------------------is_smiling-----------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.20      1.00      0.33         1\n",
      "           1       1.00      0.70      0.82        10\n",
      "         UNK       0.88      0.78      0.82         9\n",
      "\n",
      "    accuracy                           0.75        20\n",
      "   macro avg       0.69      0.83      0.66        20\n",
      "weighted avg       0.90      0.75      0.80        20\n",
      " \n",
      "\n",
      "--------------------sex-----------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           F       0.83      1.00      0.91         5\n",
      "           M       1.00      0.83      0.91         6\n",
      "         MIX       1.00      1.00      1.00         2\n",
      "         UNK       0.86      0.86      0.86         7\n",
      "\n",
      "    accuracy                           0.90        20\n",
      "   macro avg       0.92      0.92      0.92        20\n",
      "weighted avg       0.91      0.90      0.90        20\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_true = {dim: [] for dim in dims}\n",
    "y_pred = {dim: [] for dim in dims}\n",
    "##Ê†ºÂºèÔºö{'is_default_pic': ['0','0', '0', ...], \"has_person\":[]}\n",
    "\n",
    "for fname in ls_annotations.keys():\n",
    "    if fname not in auto_annotations:\n",
    "        continue\n",
    "    gt = ls_annotations[fname]\n",
    "    pred = auto_annotations[fname]\n",
    "\n",
    "    for dim in dims:\n",
    "        y_true[dim].append(gt.get(dim, \"UNK\"))\n",
    "        y_pred[dim].append(pred.get(dim, \"UNK\"))\n",
    "\n",
    "for dim in dims:\n",
    "    print(f\"--------------------{dim}-----------------------\")\n",
    "    print(classification_report(y_true[dim], y_pred[dim], zero_division=0),'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c6c195",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "81a0ee38",
   "metadata": {},
   "source": [
    "## check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8203c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Ê£ÄÊü•images, annot, pred ÁöÑkeysÊòØÂê¶ÂØπÂ∫îÔºÅÔºÅ\n",
    "# import os\n",
    "# import json\n",
    "# with open(\"annotations_SAMPLE/autoclf_predictions.json\", \"r\", encoding='utf-8')as j:\n",
    "#     predictions=json.load(j)\n",
    "# print(predictions)\n",
    "# pred_keys=predictions.keys()\n",
    "# print(len(predictions))\n",
    "\n",
    "\n",
    "# sample_keys=[k for k in os.listdir('images_SAMPLE') if k.endswith('.jpg')]\n",
    "# print(sample_keys)\n",
    "# inter=set(annot_keys).intersection(set(pred_keys))\n",
    "# print(inter)\n",
    "# print(len(inter))\n",
    "\n",
    "\n",
    "# inter=set(sample_keys).intersection(set(pred_keys))\n",
    "# print(inter)\n",
    "# print(len(inter))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acf470c",
   "metadata": {},
   "source": [
    "## eval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f704315f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33206f8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6938d525",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a5022b0",
   "metadata": {},
   "source": [
    "# APPLICATION:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6ab50f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "from utils import image_auto_annotation\n",
    "importlib.reload(image_auto_annotation)\n",
    "from utils.image_auto_annotation import autoclf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d99a239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================EMBEDDING + FEW SHOT=======================\n",
      "\n",
      "[SUCCES] labels text saved to labels/labels_text.json!\n",
      "[SUCCES] labels text-image prompt embeded by openai/clip-vit-base-patch32 saved to labels/labels_emb_txt-img.pkl!\n",
      "\n",
      "============================PREDICTION==============================\n",
      "-is_default_pic:'1'/'0'\n",
      "-type : life/pro/UNK (text prompt+few-shot)\n",
      "-is_smiling: '1'/'0'/'UNK'\n",
      "-sex: :M/F/MIX/UNK \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "predict on images...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 2659.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUCCES] Auto predictions on 20 images saved ‚Üí annotations_SAMPLE/autoclf_predictions.json!\n",
      "\n",
      "============================DETECTION===============================\n",
      "-has_person by yolo:'1'/'0',\n",
      "quality by Laplacian : high/low/UNK\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "detecting has_person and picture quality...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:02<00:00,  8.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUCCES] 20 detection results saved to annotations_SAMPLE/detections.json!\n",
      "\n",
      "==============================MERGE=============================\n",
      "merging detections and predictions...\n",
      "\n",
      "[CHECK] keys alignement: True\n",
      "[SAVE] merged and overrided auto-annotations saved in annotations_SAMPLE/auto_annotations.json!\n",
      "\n",
      "==============================EVALUATION============================\n",
      "ls_annotations : y_true,\n",
      "auto_annotations:y_pred\n",
      "\n",
      "classification report:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dimension</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>macro_f1</th>\n",
       "      <th>micro_f1</th>\n",
       "      <th>UNK_count</th>\n",
       "      <th>low_performance_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>is_default_pic</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>has_person</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.890110</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>type</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.707143</td>\n",
       "      <td>0.70</td>\n",
       "      <td>6</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>quality</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.438272</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>is_smiling</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.660131</td>\n",
       "      <td>0.75</td>\n",
       "      <td>9</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sex</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.918831</td>\n",
       "      <td>0.90</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        dimension  accuracy  macro_f1  micro_f1  UNK_count  \\\n",
       "0  is_default_pic      1.00  1.000000      1.00          0   \n",
       "1      has_person      0.90  0.890110      0.90          0   \n",
       "2            type      0.70  0.707143      0.70          6   \n",
       "3         quality      0.70  0.438272      0.70          1   \n",
       "4      is_smiling      0.75  0.660131      0.75          9   \n",
       "5             sex      0.90  0.918831      0.90          7   \n",
       "\n",
       "   low_performance_flag  \n",
       "0                 False  \n",
       "1                 False  \n",
       "2                 False  \n",
       "3                 False  \n",
       "4                 False  \n",
       "5                 False  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings-> Prediction-> Detection-> Merge-> Evaluation \n",
      "20 images in images_SAMPLE :6.60 sec! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# images_folder=\"images_SAMPLE\"\n",
    "# image_emb_path=\"embeddings_SAMPLE/emb_SAMPLE.npz\"\n",
    "# labels_text_path=\"labels/labels_text.json\"\n",
    "# labels_emb_path=\"labels/labels_emb_txt-img.pkl\"\n",
    "\n",
    "# prediction_path=\"annotations_SAMPLE/autoclf_predictions.json\"\n",
    "# detection_path=\"annotations_SAMPLE/detections.json\"\n",
    "# auto_annotations_path=\"annotations_SAMPLE/auto_annotations.json\"\n",
    "# ls_annotations_path=\"annotations_SAMPLE/ls_annotations.json\"\n",
    "\n",
    "# autoclf(images_folder,\n",
    "#             image_emb_path,\n",
    "#             labels_text_path, \n",
    "#             labels_emb_path,\n",
    "#             prediction_path,\n",
    "#             detection_path,\n",
    "#             auto_annotations_path,\n",
    "#             ls_annotations_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fa8495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================EMBEDDING + FEW SHOT=======================\n",
      "\n",
      "[SUCCES] labels text saved to labels/labels_text.json!\n",
      "[SUCCES] labels text-image prompt embeded by openai/clip-vit-base-patch32 saved to labels/labels_emb_txt-img.pkl!\n",
      "\n",
      "============================PREDICTION==============================\n",
      "-is_default_pic:'1'/'0'\n",
      "-type : life/pro/UNK (text prompt+few-shot)\n",
      "-is_smiling: '1'/'0'/'UNK'\n",
      "-sex: :M/F/MIX/UNK \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "predict on images...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 299/299 [00:00<00:00, 7904.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUCCES] Auto predictions on 299 images saved ‚Üí annotations_TEST\\predictions.json: 0.06 sec!\n",
      "\n",
      "============================DETECTION===============================\n",
      "-has_person by yolo:'1'/'0',\n",
      "quality by Laplacian : high/low/UNK\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "detecting has_person and picture quality...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 299/299 [00:31<00:00,  9.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUCCES] 299 detection results saved to annotations_TEST\\detections.json 31.60 sec!\n",
      "\n",
      "==============================MERGE=============================\n",
      "merging detections and predictions...\n",
      "\n",
      "[CHECK] keys alignement: True\n",
      "[SAVE] merged and overrided auto-annotations saved in annotations_TEST\\auto_annotations.json!\n",
      "\n",
      "Embeddings-> Prediction-> Detection-> Merge-> Evaluation \n",
      "299 images in images_TEST :35.98 sec! \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#label text-img emb‰∏çÂèòÔºå‰∏çÈúÄË¶ÅÈáçÊñ∞embedded\n",
    "labels_text_path=\"labels/labels_text.json\"\n",
    "labels_emb_path=\"labels/labels_emb_txt-img.pkl\"\n",
    "\n",
    "images_folder=\"images_TEST\"\n",
    "image_emb_path=\"embeddings/emb_TEST.npz\"\n",
    "anno_folder='annotations_TEST'\n",
    "\n",
    "prediction_path=os.path.join(anno_folder,\"predictions.json\")\n",
    "detection_path=os.path.join(anno_folder,\"detections.json\")\n",
    "auto_annotations_path=os.path.join(anno_folder,\"auto_annotations.json\")\n",
    "ls_annotations_path=None\n",
    "\n",
    "autoclf(images_folder,\n",
    "            image_emb_path,\n",
    "            labels_text_path, \n",
    "            labels_emb_path,\n",
    "            prediction_path,\n",
    "            detection_path,\n",
    "            auto_annotations_path,\n",
    "            ls_annotations_path)\n",
    "\n",
    "##clipÈùûÂ∏∏Âø´ÔºÅyoloÂÅèÊÖ¢ÔºåÂçäÂàÜÈíü„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe59da83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "airbnb_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
