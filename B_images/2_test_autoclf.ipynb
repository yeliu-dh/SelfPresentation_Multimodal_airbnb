{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b6b44d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os, sys, importlib\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "from utils import images_clf\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac681d5",
   "metadata": {},
   "source": [
    "## TEST : autoclf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35d1d85",
   "metadata": {},
   "source": [
    "## labels2emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "23d504e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## labels_text (prompt):\n",
    "#Ôºü\n",
    "is_default_pic_labels = {\n",
    "    \"1\": \"the official Airbnb default profile picture, a gray geometric human silhouette\",\n",
    "    \"0\": \"a normal user-uploaded profile picture\"\n",
    "}\n",
    "\n",
    "# has_person_labels = {\n",
    "#     \"1\": \"a photo that contains one or more people\",\n",
    "#     \"0\": \"a photo without any people\"\n",
    "# }\n",
    "# type_labels = {\n",
    "#     \"life\": \"a person shown in a real-life scene or activity, with visible environment or lifestyle context.\",\n",
    "#     \"pro\": \"a clean portrait or headshot focused mainly on the face, with little or no background information.\",\n",
    "#     \"UNK\": \"no visible person, or not enough information to determine lifestyle vs portrait.\"\n",
    "# }\n",
    "\n",
    "type_labels = {\n",
    "    \"life\": \n",
    "        \"a photo of a person in a visible daily scene or some activities\",\n",
    "    \"pro\": \n",
    "        \"a portrait or headshot,focused mainly on the face, with little or no background information.\",\n",
    "    \"UNK\": \n",
    "        \"an image without any people or cannot determine whether it is lifestyle or portrait\"\n",
    "}\n",
    "# quality_labels = {\n",
    "#     \"high\": \n",
    "#         \"a clear, high-quality photo with good lighting and sharp details\",\n",
    "#     \"low\": \n",
    "#         \"a low-quality photo with blur, noise, poor lighting or distortion\",\n",
    "#     \"UNK\": \n",
    "#         \"quality cannot be determined\"\n",
    "# }\n",
    "is_smiling_labels = {\n",
    "    \"1\": \"a person smiling visibly\",\n",
    "    \"0\": \"a person not smiling\",\n",
    "    \"UNK\": \"no person or cannot see their face\"\n",
    "}\n",
    "sex_labels = {\n",
    "    \"M\": \"a photo of a man\",\n",
    "    \"F\": \"a photo of a woman\",\n",
    "    \"MIX\": \"a photo with multiple people of mixed gender\",\n",
    "    \"UNK\": \"the gender of the person cannot be determined or no person present\"\n",
    "}\n",
    "labels_text = {\n",
    "    # \"has_person\": has_person_labels,\n",
    "    \"is_default_pic\": is_default_pic_labels,\n",
    "    \"type\": type_labels,\n",
    "    # \"quality\": quality_labels,\n",
    "    \"is_smiling\": is_smiling_labels,\n",
    "    \"sex\": sex_labels   \n",
    "}\n",
    "\n",
    "with open (\"labels/labels_text.json\",\"w\", encoding='utf-8') as f :\n",
    "    json.dump(labels_text, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0cf6dcde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\miniconda3\\envs\\airbnb_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "# embed labels :\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "device='cuda' if torch.cuda.is_available() else \"cpu\" \n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d00f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUCCES] Saved text embeddings ‚Üí labels/labels_emb.npz\n"
     ]
    }
   ],
   "source": [
    "# 1) ËØªÂèñ‰Ω†ÁöÑ labels JSON\n",
    "with open(\"labels/labels_text.json\", \"r\") as f:\n",
    "    labels_text = json.load(f)\n",
    "\n",
    "def embed_text_by_clip(text_list):\n",
    "    \"\"\"\n",
    "    text_list: list of strings\n",
    "    return: np.array of shape (len(text_list), embedding_dim)\n",
    "    label Ë¢´ÁúÅÂéªÔºåÂè™Áïô‰∏ãÂÖ∑‰ΩìÊèèËø∞\n",
    "\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        inputs = processor(text=text_list, return_tensors=\"pt\", padding=True).to(device)\n",
    "        text_features = model.get_text_features(**inputs)  # (N, 512)\n",
    "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "        return text_features.cpu().numpy().astype(\"float32\")\n",
    "\n",
    "# 2) ÁîüÊàê embedding\n",
    "labels_emb = {}\n",
    "for category, dic in labels_text.items():\n",
    "    texts = list(dic.values())  # e.g. [\"life prompt\", \"pro prompt\", \"UNK prompt\"]\n",
    "    emb = embed_text_by_clip(texts)  # shape = (num_classes, 512)\n",
    "    labels_emb[category] = emb\n",
    "\n",
    "\n",
    "# 3) ‰øùÂ≠òÂà∞Âçï‰∏™ npz Êñá‰ª∂\n",
    "np.savez(\"labels/labels_emb.npz\", **labels_emb)\n",
    "print(\"[SUCCES] Saved text embeddings ‚Üí labels/labels_emb.npz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99c9fbf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 512)\n"
     ]
    }
   ],
   "source": [
    "#Ê£ÄÊü•Ôºö\n",
    "emb = np.load(\"labels/labels_emb.npz\")\n",
    "type_emb = emb[\"type\"]       # (3, 512)\n",
    "quality_emb = emb[\"quality\"] # (3, 512)\n",
    "sex_emb = emb[\"sex\"]         # (4, 512)\n",
    "print(type_emb.shape)\n",
    "# ÂÅáËÆæÊúâ‰∏Ä‰∏™ image embedding img_emb (1, 512)\n",
    "\n",
    "# import numpy as np\n",
    "# pred_idx = np.argmax(np.dot(img_emb, type_emb.T))\n",
    "# pred_label = list(labels_text[\"type\"].keys())[pred_idx]\n",
    "# print(pred_label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d587d4ec",
   "metadata": {},
   "source": [
    "## PREDICTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e8bd5299",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "predict on images...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 1224.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Auto predictions on 20 images saved ‚Üí annotations_SAMPLE/autoclf_predictions.json: 0.03 sec!\n",
      "{'102571900.jpg': {'is_default_pic': '0', 'type': 'life', 'is_smiling': '1', 'sex': 'F'}, '106294215.jpg': {'is_default_pic': '0', 'type': 'UNK', 'is_smiling': 'UNK', 'sex': 'UNK'}, '106365215.jpg': {'is_default_pic': '0', 'type': 'life', 'is_smiling': '1', 'sex': 'M'}, '137154154.jpg': {'is_default_pic': '0', 'type': 'life', 'is_smiling': 'UNK', 'sex': 'F'}, '212791574.jpg': {'is_default_pic': '0', 'type': 'UNK', 'is_smiling': '0', 'sex': 'UNK'}, '2379345.jpg': {'is_default_pic': '0', 'type': 'UNK', 'is_smiling': '0', 'sex': 'UNK'}, '24654560.jpg': {'is_default_pic': '0', 'type': 'life', 'is_smiling': 'UNK', 'sex': 'MIX'}, '2798386.jpg': {'is_default_pic': '0', 'type': 'pro', 'is_smiling': '1', 'sex': 'M'}, '28470251.jpg': {'is_default_pic': '0', 'type': 'pro', 'is_smiling': '1', 'sex': 'F'}, '32741638.jpg': {'is_default_pic': '0', 'type': 'UNK', 'is_smiling': '1', 'sex': 'F'}, '336591839.jpg': {'is_default_pic': '1', 'type': 'life', 'is_smiling': '1', 'sex': 'UNK'}, '425502119.jpg': {'is_default_pic': '0', 'type': 'UNK', 'is_smiling': 'UNK', 'sex': 'UNK'}, '517697918.jpg': {'is_default_pic': '0', 'type': 'UNK', 'is_smiling': 'UNK', 'sex': 'UNK'}, '52438163.jpg': {'is_default_pic': '0', 'type': 'life', 'is_smiling': '0', 'sex': 'MIX'}, '52801103.jpg': {'is_default_pic': '0', 'type': 'UNK', 'is_smiling': 'UNK', 'sex': 'UNK'}, '553099349.jpg': {'is_default_pic': '0', 'type': 'life', 'is_smiling': '0', 'sex': 'M'}, '57226046.jpg': {'is_default_pic': '0', 'type': 'pro', 'is_smiling': '0', 'sex': 'M'}, '71320446.jpg': {'is_default_pic': '0', 'type': 'pro', 'is_smiling': '1', 'sex': 'M'}, '873444.jpg': {'is_default_pic': '0', 'type': 'life', 'is_smiling': '1', 'sex': 'F'}, '88933385.jpg': {'is_default_pic': '0', 'type': 'life', 'is_smiling': 'UNK', 'sex': 'F'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "start_time=time.time()\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# Êñá‰ª∂Ë∑ØÂæÑ\n",
    "# ------------------------\n",
    "image_emb_path = \"embeddings_SAMPLE/emb_SAMPLE.npz\"\n",
    "text_emb_path = \"labels/labels_emb.npz\"\n",
    "text_json_path = \"labels/labels_text.json\"\n",
    "output_json_path = \"annotations_SAMPLE/autoclf_predictions.json\"\n",
    "\n",
    "# ------------------------\n",
    "# ËØªÂèñ embeddings\n",
    "# ------------------------\n",
    "image_embs = np.load(image_emb_path)  # keys: \"host_id.jpg\"\n",
    "text_embs_np = np.load(text_emb_path)\n",
    "with open(text_json_path, \"r\") as f:\n",
    "    labels_text = json.load(f)\n",
    "default_pic_emb=image_embs[\"336591839.jpg\"]\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# È¢ÑÊµãÂáΩÊï∞\n",
    "# ------------------------\n",
    "def is_default_pic(image_emb, default_pic_emb, threshold=0.95):\n",
    "    \"\"\"\n",
    "    image_emb: np.array (512,)\n",
    "    default_emb: np.array (512,)\n",
    "    threshold: cosine similarity threshold\n",
    "    \"\"\"\n",
    "    sim = image_emb @ default_pic_emb  # cosine similarity, embeddings Â∑≤Áªè L2-normalized\n",
    "    return \"1\" if sim >= threshold else \"0\"\n",
    "\n",
    "def zero_shot_predict(img_emb, text_emb_dict):\n",
    "    \"\"\"\n",
    "    img_emb: np.array (512,)\n",
    "    text_emb_dict: np.array (num_classes, 512)\n",
    "    \"\"\"\n",
    "    sims = img_emb @ text_emb_dict.T        # cosine similarity\n",
    "    idx = np.argmax(sims)\n",
    "    return idx, sims\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# ÈÅçÂéÜÊØèÂº†ÂõæÁâá\n",
    "# ------------------------\n",
    "predictions = {}  # Â≠òÂÇ®ÁªìÊûú\n",
    "for fname in tqdm(image_embs.files, desc='predict on images...'):\n",
    "    img_emb = image_embs[fname]\n",
    "\n",
    "    pred = {}\n",
    "    for category, text_emb_np in text_embs_np.items():\n",
    "        #ÂØπÊØè‰∏Ä‰∏™Áª¥Â∫¶ËøõË°åÂàÜÁ±ªÔºåÊ∑ªÂä†Âà∞predÁªìÊûú‰∏≠Ôºö\n",
    "        if category == \"is_default_pic\":\n",
    "            pred_label = is_default_pic(img_emb, default_pic_emb)\n",
    "        else:\n",
    "            idx, sims = zero_shot_predict(img_emb, text_emb_np)\n",
    "            label_keys = list(labels_text[category].keys())\n",
    "            pred_label = label_keys[idx]\n",
    "\n",
    "        pred[category] = pred_label\n",
    "        \n",
    "        #ÂÖ®ÈÉ®Âà§Êñ≠ÂÆå‰πãÂêéÁöÑÊ£ÄÊü•Ôºö\n",
    "        if pred[\"is_default_pic\"]==1:\n",
    "            pred = { \n",
    "                \"type\": \"UNK\",\n",
    "                # \"quality\": \"low\",\n",
    "                \"is_smiling\": \"UNK\",\n",
    "                \"sex\": \"UNK\",\n",
    "                # \"has_person\": \"0\",\n",
    "                \"is_default_pic\": \"1\"\n",
    "                            } \n",
    "    predictions[fname] = pred\n",
    "\n",
    "# ------------------------\n",
    "# ‰øùÂ≠ò JSON\n",
    "# ------------------------\n",
    "with open(output_json_path, \"w\") as f:\n",
    "    json.dump(predictions, f, indent=2)\n",
    "\n",
    "end_time=time.time()\n",
    "print(f\"‚úÖ Auto predictions on {len(image_embs)} images saved ‚Üí {output_json_path}: {end_time-start_time:.2f} sec!\")\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e75c5404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "## evaluation of clip:\n",
    "import json\n",
    "with open(\"annotations_SAMPLE\\ls_annotations.json\", \"r\", encoding='utf-8')as j:\n",
    "    annotations=json.load(j)\n",
    "annot_keys=[f['data']['filename'] for f in annotations]\n",
    "print(len(annotations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7250143a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'102571900.jpg': {'type': 'life', 'quality': 'low', 'is_smiling': '1', 'sex': 'F', 'has_person': '0', 'is_default_pic': '0'}, '106294215.jpg': {'type': 'UNK', 'quality': 'high', 'is_smiling': 'UNK', 'sex': 'UNK', 'has_person': '0', 'is_default_pic': '0'}, '106365215.jpg': {'type': 'life', 'quality': 'low', 'is_smiling': '1', 'sex': 'M', 'has_person': '0', 'is_default_pic': '0'}, '137154154.jpg': {'type': 'life', 'quality': 'high', 'is_smiling': 'UNK', 'sex': 'F', 'has_person': '0', 'is_default_pic': '0'}, '212791574.jpg': {'type': 'UNK', 'quality': 'low', 'is_smiling': '0', 'sex': 'UNK', 'has_person': '1', 'is_default_pic': '0'}, '2379345.jpg': {'type': 'UNK', 'quality': 'low', 'is_smiling': '0', 'sex': 'UNK', 'has_person': '0', 'is_default_pic': '0'}, '24654560.jpg': {'type': 'life', 'quality': 'low', 'is_smiling': 'UNK', 'sex': 'MIX', 'has_person': '1', 'is_default_pic': '0'}, '2798386.jpg': {'type': 'pro', 'quality': 'low', 'is_smiling': '1', 'sex': 'M', 'has_person': '0', 'is_default_pic': '0'}, '28470251.jpg': {'type': 'pro', 'quality': 'low', 'is_smiling': '1', 'sex': 'F', 'has_person': '1', 'is_default_pic': '0'}, '32741638.jpg': {'type': 'UNK', 'quality': 'low', 'is_smiling': '1', 'sex': 'F', 'has_person': '0', 'is_default_pic': '0'}, '336591839.jpg': {'type': 'life', 'quality': 'low', 'is_smiling': '1', 'sex': 'UNK', 'has_person': '1', 'is_default_pic': '1'}, '425502119.jpg': {'type': 'UNK', 'quality': 'low', 'is_smiling': 'UNK', 'sex': 'UNK', 'has_person': '1', 'is_default_pic': '0'}, '517697918.jpg': {'type': 'UNK', 'quality': 'high', 'is_smiling': 'UNK', 'sex': 'UNK', 'has_person': '0', 'is_default_pic': '0'}, '52438163.jpg': {'type': 'life', 'quality': 'low', 'is_smiling': '0', 'sex': 'MIX', 'has_person': '0', 'is_default_pic': '0'}, '52801103.jpg': {'type': 'UNK', 'quality': 'low', 'is_smiling': 'UNK', 'sex': 'UNK', 'has_person': '0', 'is_default_pic': '0'}, '553099349.jpg': {'type': 'life', 'quality': 'low', 'is_smiling': '0', 'sex': 'M', 'has_person': '0', 'is_default_pic': '0'}, '57226046.jpg': {'type': 'pro', 'quality': 'high', 'is_smiling': '0', 'sex': 'M', 'has_person': '0', 'is_default_pic': '0'}, '71320446.jpg': {'type': 'pro', 'quality': 'low', 'is_smiling': '1', 'sex': 'M', 'has_person': '0', 'is_default_pic': '0'}, '873444.jpg': {'type': 'life', 'quality': 'low', 'is_smiling': '1', 'sex': 'F', 'has_person': '0', 'is_default_pic': '0'}, '88933385.jpg': {'type': 'life', 'quality': 'low', 'is_smiling': 'UNK', 'sex': 'F', 'has_person': '0', 'is_default_pic': '0'}}\n",
      "20\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980d74ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['102571900.jpg', '106294215.jpg', '106365215.jpg', '137154154.jpg', '212791574.jpg', '2379345.jpg', '24654560.jpg', '2798386.jpg', '28470251.jpg', '32741638.jpg', '336591839.jpg', '425502119.jpg', '517697918.jpg', '52438163.jpg', '52801103.jpg', '553099349.jpg', '57226046.jpg', '71320446.jpg', '873444.jpg', '88933385.jpg']\n",
      "{'336591839.jpg', '88933385.jpg', '2379345.jpg', '102571900.jpg', '57226046.jpg', '212791574.jpg', '28470251.jpg', '71320446.jpg', '106365215.jpg', '106294215.jpg', '52801103.jpg', '425502119.jpg', '873444.jpg', '24654560.jpg', '553099349.jpg', '137154154.jpg', '32741638.jpg', '2798386.jpg', '517697918.jpg', '52438163.jpg'}\n",
      "20\n",
      "{'336591839.jpg', '88933385.jpg', '2379345.jpg', '102571900.jpg', '57226046.jpg', '212791574.jpg', '28470251.jpg', '71320446.jpg', '106365215.jpg', '106294215.jpg', '52801103.jpg', '425502119.jpg', '873444.jpg', '24654560.jpg', '553099349.jpg', '137154154.jpg', '32741638.jpg', '2798386.jpg', '517697918.jpg', '52438163.jpg'}\n",
      "20\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a6329c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a6a709b0",
   "metadata": {},
   "source": [
    "## DETECTION:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f4054833",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "detecting has_person and picture quality...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:01<00:00, 13.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUCCES] Detection results saved to annotations_SAMPLE/detections.json :1.47 sec!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'102571900.jpg': {'has_person': '1', 'quality': 'low'},\n",
       " '106294215.jpg': {'has_person': '0', 'quality': 'low'},\n",
       " '106365215.jpg': {'has_person': '1', 'quality': 'low'},\n",
       " '137154154.jpg': {'has_person': '1', 'quality': 'low'},\n",
       " '212791574.jpg': {'has_person': '0', 'quality': 'low'},\n",
       " '2379345.jpg': {'has_person': '0', 'quality': 'low'},\n",
       " '24654560.jpg': {'has_person': '1', 'quality': 'low'},\n",
       " '2798386.jpg': {'has_person': '1', 'quality': 'low'},\n",
       " '28470251.jpg': {'has_person': '1', 'quality': 'low'},\n",
       " '32741638.jpg': {'has_person': '0', 'quality': 'low'},\n",
       " '336591839.jpg': {'has_person': '0', 'quality': 'low'},\n",
       " '425502119.jpg': {'has_person': '0', 'quality': 'low'},\n",
       " '517697918.jpg': {'has_person': '1', 'quality': 'low'},\n",
       " '52438163.jpg': {'has_person': '1', 'quality': 'low'},\n",
       " '52801103.jpg': {'has_person': '0', 'quality': 'low'},\n",
       " '553099349.jpg': {'has_person': '1', 'quality': 'low'},\n",
       " '57226046.jpg': {'has_person': '1', 'quality': 'low'},\n",
       " '71320446.jpg': {'has_person': '1', 'quality': 'low'},\n",
       " '873444.jpg': {'has_person': '1', 'quality': 'low'},\n",
       " '88933385.jpg': {'has_person': '0', 'quality': 'low'}}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "# -------------------------------------------------\n",
    "# 1. Âä†ËΩΩ YOLO Ê®°ÂûãÔºàÊúÄËΩªÈáè CPU/GPU ÈÄöÂêÉÔºâ\n",
    "# -------------------------------------------------\n",
    "model = YOLO(\"yolov8n.pt\")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 2. quality Âà§Êñ≠ÔºàÂü∫‰∫éÊúÄÁü≠Ëæπ pxÔºâ\n",
    "# -------------------------------------------------\n",
    "def get_quality_label(img_path):\n",
    "    try:\n",
    "        img = Image.open(img_path)\n",
    "        w, h = img.size\n",
    "        short_edge = min(w, h)\n",
    "\n",
    "        # Ê†πÊçÆ Airbnb Â§¥ÂÉèÁâπÁÇπÔºö200px ‰ª•‰∏ãÂá†‰πéËÇØÂÆö low\n",
    "        if short_edge >= 300:\n",
    "            return \"high\"\n",
    "        else:\n",
    "            return \"low\"\n",
    "    except:\n",
    "        return \"UNK\"\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 3. YOLO Âà§Êñ≠ has_person\n",
    "# -------------------------------------------------\n",
    "def detect_has_person(img_path):\n",
    "    try:\n",
    "        # results = model(img_path)[0]  # first result\n",
    "        results = model(img_path, verbose=False)[0]  # üëà ÂÖ≥Èó≠ÊâÄÊúâÊó•ÂøóËæìÂá∫\n",
    "        for box in results.boxes:\n",
    "            cls = int(box.cls[0])\n",
    "            if results.names[cls] == \"person\":\n",
    "                return \"1\"\n",
    "        return \"0\"\n",
    "    except:\n",
    "        return \"UNK\"\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 4. main pipeline\n",
    "# -------------------------------------------------\n",
    "def detect_person_and_quality(\n",
    "        images_folder=\"images_SAMPLE\",\n",
    "        save_json=\"annotations_SAMPLE/detections.json\"\n",
    "    ):\n",
    "    start_time=time.time()\n",
    "\n",
    "    os.makedirs(os.path.dirname(save_json), exist_ok=True)\n",
    "    records = {}\n",
    "\n",
    "    image_files = [f for f in os.listdir(images_folder) if f.lower().endswith((\".jpg\",\".jpeg\",\".png\"))]\n",
    "\n",
    "    # print(f\"[INFO] Found {len(image_files)} images in {images_folder}\")\n",
    "\n",
    "    for fname in tqdm(image_files, desc=\"detecting has_person and picture quality...\"):\n",
    "        fpath = os.path.join(images_folder, fname)\n",
    "\n",
    "        # YOLO: detect person\n",
    "        has_person = detect_has_person(fpath)\n",
    "\n",
    "        # quality: size-based\n",
    "        quality = get_quality_label(fpath)\n",
    "\n",
    "        records[fname] = {\n",
    "            \"has_person\": has_person,\n",
    "            \"quality\": quality\n",
    "        }\n",
    "\n",
    "        # print(f\"[OK] {fname} ‚Üí has_person={has_person}, quality={quality}\")\n",
    "\n",
    "    # save json\n",
    "    with open(save_json, \"w\") as f:\n",
    "        json.dump(records, f, indent=2)\n",
    "    end_time=time.time()\n",
    "\n",
    "    print(f\"[SUCCES] Detection results saved to {save_json} :{end_time-start_time:.2f} sec!\")\n",
    "    return records\n",
    "\n",
    "# # -------------------------------------------------\n",
    "# # Run\n",
    "# # -------------------------------------------------\n",
    "# if __name__ == \"__main__\":\n",
    "#     run_yolo_pipeline()\n",
    "\n",
    "\n",
    "detect_person_and_quality(\n",
    "        images_folder=\"images_SAMPLE\",\n",
    "        save_json=\"annotations_SAMPLE/detections.json\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f54609f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "81a0ee38",
   "metadata": {},
   "source": [
    "## check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8203c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ê£ÄÊü•images, annot, pred ÁöÑkeysÊòØÂê¶ÂØπÂ∫îÔºÅÔºÅ\n",
    "import os\n",
    "import json\n",
    "with open(\"annotations_SAMPLE/autoclf_predictions.json\", \"r\", encoding='utf-8')as j:\n",
    "    predictions=json.load(j)\n",
    "print(predictions)\n",
    "pred_keys=predictions.keys()\n",
    "print(len(predictions))\n",
    "\n",
    "\n",
    "sample_keys=[k for k in os.listdir('images_SAMPLE') if k.endswith('.jpg')]\n",
    "print(sample_keys)\n",
    "inter=set(annot_keys).intersection(set(pred_keys))\n",
    "print(inter)\n",
    "print(len(inter))\n",
    "\n",
    "\n",
    "inter=set(sample_keys).intersection(set(pred_keys))\n",
    "print(inter)\n",
    "print(len(inter))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acf470c",
   "metadata": {},
   "source": [
    "## eval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f704315f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33206f8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6938d525",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a5022b0",
   "metadata": {},
   "source": [
    "# APPLICATION:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab50f58",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "airbnb_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
