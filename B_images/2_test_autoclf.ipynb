{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6b44d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os, sys, importlib\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "from utils import images_clf\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac681d5",
   "metadata": {},
   "source": [
    "## TEST : autoclf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35d1d85",
   "metadata": {},
   "source": [
    "## labels2emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d504e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## labels_text (prompt):\n",
    "#？\n",
    "is_default_pic_labels = {\n",
    "    \"1\": \"the official Airbnb default profile picture, a gray geometric human silhouette\",\n",
    "    \"0\": \"a normal user-uploaded profile picture\"\n",
    "}\n",
    "\n",
    "has_person_labels = {\n",
    "    \"1\": \"a photo that contains one or more people\",\n",
    "    \"0\": \"a photo without any people\"\n",
    "}\n",
    "# type_labels = {\n",
    "#     \"life\": \"a person shown in a real-life scene or activity, with visible environment or lifestyle context.\",\n",
    "#     \"pro\": \"a clean portrait or headshot focused mainly on the face, with little or no background information.\",\n",
    "#     \"UNK\": \"no visible person, or not enough information to determine lifestyle vs portrait.\"\n",
    "# }\n",
    "\n",
    "type_labels = {\n",
    "    \"life\": \n",
    "        \"a photo of a person in a visible daily scene or some activities\",\n",
    "    \"pro\": \n",
    "        \"a portrait or headshot,focused mainly on the face, with little or no background information.\",\n",
    "    \"UNK\": \n",
    "        \"an image without any people or cannot determine whether it is lifestyle or portrait\"\n",
    "}\n",
    "quality_labels = {\n",
    "    \"high\": \n",
    "        \"a clear, high-quality photo with good lighting and sharp details\",\n",
    "    \"low\": \n",
    "        \"a low-quality photo with blur, noise, poor lighting or distortion\",\n",
    "    \"UNK\": \n",
    "        \"quality cannot be determined\"\n",
    "}\n",
    "is_smiling_labels = {\n",
    "    \"1\": \"a person smiling visibly\",\n",
    "    \"0\": \"a person not smiling\",\n",
    "    \"UNK\": \"no person or cannot see their face\"\n",
    "}\n",
    "sex_labels = {\n",
    "    \"M\": \"a photo of a man\",\n",
    "    \"F\": \"a photo of a woman\",\n",
    "    \"MIX\": \"a photo with multiple people of mixed gender\",\n",
    "    \"UNK\": \"the gender of the person cannot be determined or no person present\"\n",
    "}\n",
    "labels_text = {\n",
    "    \"type\": type_labels,\n",
    "    \"quality\": quality_labels,\n",
    "    \"is_smiling\": is_smiling_labels,\n",
    "    \"sex\": sex_labels,\n",
    "    \"has_person\": has_person_labels,\n",
    "    \"is_default_pic\": is_default_pic_labels\n",
    "}\n",
    "\n",
    "with open (\"labels/labels_text.json\",\"w\", encoding='utf-8') as f :\n",
    "    json.dump(labels_text, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0cf6dcde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 12e9ac5f-7195-4abf-94f9-6b48dfe3f542)')' thrown while requesting HEAD https://huggingface.co/openai/clip-vit-base-patch32/resolve/main/config.json\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    }
   ],
   "source": [
    "# embed labels :\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "device='cuda' if torch.cuda.is_available() else \"cpu\" \n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09d00f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUCCES] Saved text embeddings → labels/labels_emb.npz\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1) 读取你的 labels JSON\n",
    "with open(\"labels/labels_text.json\", \"r\") as f:\n",
    "    labels_text = json.load(f)\n",
    "\n",
    "def embed_text_by_clip(text_list):\n",
    "    \"\"\"\n",
    "    text_list: list of strings\n",
    "    return: np.array of shape (len(text_list), embedding_dim)\n",
    "    label 被省去，只留下具体描述\n",
    "\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        inputs = processor(text=text_list, return_tensors=\"pt\", padding=True).to(device)\n",
    "        text_features = model.get_text_features(**inputs)  # (N, 512)\n",
    "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "        return text_features.cpu().numpy().astype(\"float32\")\n",
    "\n",
    "\n",
    "# 2) 生成 embedding\n",
    "labels_emb = {}\n",
    "for category, dic in labels_text.items():\n",
    "    texts = list(dic.values())  # e.g. [\"life prompt\", \"pro prompt\", \"UNK prompt\"]\n",
    "    emb = embed_text_by_clip(texts)  # shape = (num_classes, 512)\n",
    "    labels_emb[category] = emb\n",
    "\n",
    "\n",
    "# 3) 保存到单个 npz 文件\n",
    "np.savez(\"labels/labels_emb.npz\", **labels_emb)\n",
    "print(\"[SUCCES] Saved text embeddings → labels/labels_emb.npz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c9fbf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 512)\n"
     ]
    }
   ],
   "source": [
    "#检查：\n",
    "emb = np.load(\"labels/labels_emb.npz\")\n",
    "type_emb = emb[\"type\"]       # (3, 512)\n",
    "quality_emb = emb[\"quality\"] # (3, 512)\n",
    "sex_emb = emb[\"sex\"]         # (4, 512)\n",
    "print(type_emb.shape)\n",
    "# 假设有一个 image embedding img_emb (1, 512)\n",
    "\n",
    "# import numpy as np\n",
    "# pred_idx = np.argmax(np.dot(img_emb, type_emb.T))\n",
    "# pred_label = list(labels_text[\"type\"].keys())[pred_idx]\n",
    "# print(pred_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b4c258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction：\n",
    "\n",
    "def zero_shot_predict(label_type, image_emb, text_emb_dict):\n",
    "    \"\"\"\n",
    "    image_emb: shape (512,)\n",
    "    text_emb_dict: {\"life\": (512,), \"pro\": (512,), ...}\n",
    "    \"\"\"\n",
    "    labels = list(text_emb_dict.keys())\n",
    "    text_embs = np.stack([text_emb_dict[k] for k in labels])  # (K, 512)\n",
    "\n",
    "    # cosine similarity\n",
    "    scores = image_emb @ text_embs.T\n",
    "    best = labels[np.argmax(scores)]\n",
    "    return best, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bd5299",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266d2718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# 开始自动分类\n",
    "# ---------------------------------------------------\n",
    "pred = {}\n",
    "\n",
    "for fname, img_emb in emb_dict.items():\n",
    "    result = {}\n",
    "\n",
    "    # -----------------------------\n",
    "    # (A) has_person\n",
    "    # -----------------------------\n",
    "    zlabel, score = zshot_predict(img_emb, [\"person\", \"no_person\"])\n",
    "    result[\"has_person\"] = 1 if zlabel == \"person\" else 0\n",
    "\n",
    "    # -----------------------------\n",
    "    # (B) is_default_pic（你的要求）\n",
    "    # -----------------------------\n",
    "    sim_default = sim(img_emb, default_emb)\n",
    "    result[\"is_default_pic\"] = 1 if sim_default > 0.94 else 0\n",
    "\n",
    "    # -----------------------------\n",
    "    # (C) type\n",
    "    # -----------------------------\n",
    "    if result[\"has_person\"] == 0:\n",
    "        result[\"type\"] = \"UNK\"\n",
    "    else:\n",
    "        zlabel, _ = zshot_predict(img_emb, [\"life\", \"pro\"])\n",
    "        result[\"type\"] = zlabel\n",
    "\n",
    "    # -----------------------------\n",
    "    # (D) is_smiling\n",
    "    # -----------------------------\n",
    "    if result[\"has_person\"] == 0:\n",
    "        result[\"is_smiling\"] = \"UNK\"\n",
    "    else:\n",
    "        zlabel, _ = zshot_predict(img_emb, [\"smiling\", \"not_smiling\"])\n",
    "        result[\"is_smiling\"] = \"1\" if zlabel == \"smiling\" else \"0\"\n",
    "\n",
    "    # -----------------------------\n",
    "    # (E) sex\n",
    "    # -----------------------------\n",
    "    if result[\"has_person\"] == 0:\n",
    "        result[\"sex\"] = \"UNK\"\n",
    "    else:\n",
    "        zlabel, _ = zshot_predict(img_emb, [\"man\", \"woman\", \"mixed\"])\n",
    "        if zlabel == \"man\":\n",
    "            result[\"sex\"] = \"M\"\n",
    "        elif zlabel == \"woman\":\n",
    "            result[\"sex\"] = \"F\"\n",
    "        else:\n",
    "            result[\"sex\"] = \"MIX\"\n",
    "\n",
    "    # -----------------------------\n",
    "    # (F) quality\n",
    "    # -----------------------------\n",
    "    if result[\"has_person\"] == 0:\n",
    "        result[\"quality\"] = \"UNK\"\n",
    "    else:\n",
    "        zlabel, _ = zshot_predict(img_emb, [\"high_quality\", \"low_quality\"])\n",
    "        result[\"quality\"] = \"high\" if zlabel == \"high_quality\" else \"low\"\n",
    "\n",
    "    pred[fname] = result\n",
    "\n",
    "\n",
    "# 保存结果\n",
    "with open(\"annotations_SAMPLE/autoclf_predictions.json\", \"w\") as f:\n",
    "    json.dump(pred, f, indent=2)\n",
    "\n",
    "print(\"✓ AutoCLF 预测完成！已保存至 autoclf_predictions.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7250143a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980d74ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a6329c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4054833",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "airbnb_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
